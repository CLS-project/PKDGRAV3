			       pkdgrav3
			 September 24, 2018
		      http://www.pkdgrav.org/

		Douglas Potter <douglas.potter@uzh.ch>
		Joachim Stadel <stadel@physik.uzh.ch>

Quick Start
-----------

The pkdgrav3 code now uses the "cmake" build system. It is recommended to
use an "out of source" build. The easiest way to accomplish this is to
create a subdirectory in the pkdgrav3 source directory:

    mkdir build
    cd build
    cmake ..
    make

This will build a single executable "pkdgrav3" and other utility programs.

Prerequisites
-------------

CMake - cmake build system

    Most modern systems already have cmake installed. Pkdgrav3 requires
    version 3.1 or newer of cmake. You can check with "cmake --version":

        pkdgrav3:~> cmake --version
        cmake version 3.5.2

    If you need a more recent version is can be found at:

        https://cmake.org/

GSL - The GNU Scientific Library

    This library is usually available on HPC systems, but if not it must be
    downloaded and compiled, and can be found at this URL.

        https://www.gnu.org/software/gsl/

    pkdgrav3 will locate the GSL installation by invoking gsl-config, so make
    sure that it is in your PATH. Alternatively, you can tell CMake where to
    find it by defining GSL_ROOT_ROOT:

        cmake -DGSL_ROOT_DIR=/opt/gsl/2.5

FFTW - Fast Fourier Transform Library (optional)

    If FFTW is available then two advanced features are enabled in pkdgrav3.
      1. Initial Condition Generation, and,
      2. Power spectrum measurement

    If is is not available on your system it can be obtained from:

        http://www.fftw.org/

    If CMake does not automatically find FFTW then you can define FFTW_ROOT_ROOT:

        cmake -DFFTW_ROOT=/path/to/fftw

CUDA

    If your system has a CUDA capable GPU then pkdgrav3 can use it.
    The necessary toolkits can be downloaded from nVidia.

	https://developer.nvidia.com/cuda-downloads

Build
-----

Once CMake has been run to produce a Makefile and associated files,
the "make" command is used to build the program, as in:

    make

The build can be done in parallel so if you are on, for example,
a 16 core machine, the build process can be sped up with:

    make -j 16

Running
-------

    pkdgrav3
    --------
    This version is run using the MPI system on the cluster in question.
    Normally this involves a special command (often "mpirun" or "mpiexec"),
    for example:

	mpiexec pkdgrav3 simfile.par

    Consult your cluster documentation on how to run MPI programs.


IA annotations
======

For the CUDA compiler, the GNU compiler must be older than 6.0
This is the command used for preparing and compiling my version, on virus

      mkdir build
      cd build
      CC=/scratch/isaacaa/opt/gcc53/bin/gcc CXX=/scratch/isaacaa/opt/gcc53/bin/g++ cmake -DFFTW_ROOT=/scratch/isaacaa/opt/fftw3  ..
      make

New compile-time flags
-------

These flags are designed to avoid plenty of if/else statements in critical parts of the code, 
as well as activating some parts of the code which, in general, will be disabled 

In the following the implemented flags will be described, to turn them on/off:

      cmake -D<FLAG>=on/off ..

CAUTION! If you first do `cmake -D<FLAG>=on ..` and then `cmake ..`, the `<FLAG>` will be still on! You can set it off explicitly or remove the build folder to reset the configuration.

### Numerical method

  * `USE_MFM` (default=off):
      Use the Meshless finite mass method for the hydrodynamics. As today, this does not, for example, avoid the storage of the 
      mass flux (which is always zero), thus can be further optimized

#### Limiters
  * `LIMITER_BARTH` (default=off)
      Apply the Barth-Jespersen limiter to spatial gradients. This is quite a restrictive limiter.

  * `LIMITER_CONDBARTH` (default=off)
      Apply the conditioned Barth-Jespersen limiter to the spatial gradients. 
      It is less restrictive as it takes into account the local ordering of the particle to relax the max/min allowed values.

#### Smoothing length computation

In the default case, the smoothing length is computed such that a estimated "number" of neighbours is maintained constant.
This estimation is computed using the local number density, and can be severely biased in simulation with high density contrasts and gradients.
This leads to particles with really low (less than 5) neighbours. This situation ends up giving clearly unphysical results.
To avoid this situation we have added the two following options. These are only needed for cosmological simulations, and may crash in some basic test as usually the particles are initialized in a regular lattice.

  * `FIXED_NSMOOTH_STRICT` (default=off):
    Computes the distance to the N-th closest neighbours and set the smoothing length such that that particle is the further in the compact support.
      
  * `FIXED_NSMOOTH_RELAXED` (default=off):
    This is similar to the default approach but instead of estimating the number of neighbours, it takes the actual quantity of neighbours for computing the smoothing length.

We have not found any significant difference among the latter approaches.

### Dimensions
  * `FORCE_2D` (default=off):
      It set the fluxes in the z-direction to zero and ignores interactions among particles with different positions in the z-axis.
      This, however, does not change the neighbor search or the tree construction, which is still done in 3D and thus is highly inefficient!

  * `FORCE_1D` (default=off):
      It set the fluxes in the z and y directions to zero and ignores interactions among particles with different positions in the z-axis.
      This, however, does not change the neighbor search or the tree construction, which is still done in 3D and thus is highly inefficient!

### Cooling
  * `COOLING` (default=off):
      It allows for the tracking of different chemical species and H/He+metal cooling following Wiersma+2009.
      The code for this part has been adapted from the EAGLE model inside the SWIFT code.
      The Wiersma+2009 HDF5 tables are needed.


### Others
  * `MAKE_GLASS` (default=off)
      Flag for creating glass-like distribution within an arbitrary-sized box.
      This make use of the Lloyd's algorithm but without building the Voronoi mesh.
      Usually ~100 iterations is enough for convergence.

  * `REGULARIZE_MESH` (default=off)
      WIP. Try to maintain a regular distribution of points to increase accuracy in the spatial gradient.
      Only valid for MFV.
